{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b53460f7",
   "metadata": {},
   "source": [
    "## Import and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb75feb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current working directory is: /home/thun/Documents/python_pj/accent_vn/notebooks\n",
      "The current working directory is: /home/thun/Documents/python_pj/accent_vn\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "print(f\"The current working directory is: {current_directory}\")\n",
    "\n",
    "os.chdir(\"..\")\n",
    "current_directory = os.getcwd()\n",
    "print(f\"The current working directory is: {current_directory}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "110724a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thun/Documents/python_pj/accent_vn/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "ds_file = \"data/vimd_dataset.pkl\"\n",
    "\n",
    "\n",
    "def load_dataset(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "        ds = pickle.load(file)\n",
    "    return ds\n",
    "\n",
    "ds = load_dataset(ds_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8ca4966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7.0\n"
     ]
    }
   ],
   "source": [
    "import torchcodec\n",
    "print(torchcodec.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab755582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cu128\n",
      "12.8\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39898e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['region', 'province_code', 'province_name', 'filename', 'text', 'speakerID', 'gender', 'audio'],\n",
      "        num_rows: 15023\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['region', 'province_code', 'province_name', 'filename', 'text', 'speakerID', 'gender', 'audio'],\n",
      "        num_rows: 2026\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['region', 'province_code', 'province_name', 'filename', 'text', 'speakerID', 'gender', 'audio'],\n",
      "        num_rows: 1900\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "826ee074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'north': 5913, 'central': 4705, 'south': 4405})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Audio\n",
    "from collections import Counter\n",
    "\n",
    "# trecast audio no decoding\n",
    "ds_no_audio = ds.cast_column(\"audio\", Audio(decode=False))\n",
    "\n",
    "# region\n",
    "regions = [r.strip().lower() for r in ds_no_audio[\"train\"][\"region\"]]\n",
    "print(Counter(regions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1453999e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique provinces:  63\n",
      "caobang: 287\n",
      "nghean: 280\n",
      "quangbinh: 280\n",
      "danang: 271\n",
      "thaibinh: 265\n",
      "binhthuan: 262\n",
      "hagiang: 260\n",
      "quangngai: 260\n",
      "binhphuoc: 260\n",
      "bariavungtau: 255\n",
      "gialai: 252\n",
      "thainguyen: 251\n",
      "quangninh: 249\n",
      "binhduong: 249\n",
      "phutho: 248\n",
      "hanoi: 248\n",
      "hochiminh: 246\n",
      "ninhthuan: 246\n",
      "haugiang: 245\n",
      "daklak: 244\n",
      "daknong: 244\n",
      "longan: 243\n",
      "phuyen: 242\n",
      "kontum: 242\n",
      "haiphong: 241\n",
      "laocai: 241\n",
      "sonla: 241\n",
      "binhdinh: 241\n",
      "langson: 240\n",
      "namdinh: 240\n",
      "thuathienhue: 240\n",
      "lamdong: 239\n",
      "hatinh: 238\n",
      "tayninh: 237\n",
      "thanhhoa: 236\n",
      "camau: 236\n",
      "laichau: 235\n",
      "quangtri: 233\n",
      "yenbai: 232\n",
      "haiduong: 231\n",
      "bentre: 231\n",
      "quangnam: 230\n",
      "backan: 229\n",
      "tiengiang: 228\n",
      "vinhlong: 226\n",
      "dienbien: 225\n",
      "khanhhoa: 225\n",
      "travinh: 225\n",
      "tuyenquang: 224\n",
      "baclieu: 224\n",
      "hungyen: 221\n",
      "bacninh: 221\n",
      "hoabinh: 220\n",
      "angiang: 220\n",
      "kiengiang: 220\n",
      "ninhbinh: 218\n",
      "dongnai: 218\n",
      "dongthap: 217\n",
      "bacgiang: 217\n",
      "soctrang: 216\n",
      "vinhphuc: 215\n",
      "hanam: 214\n",
      "cantho: 209\n"
     ]
    }
   ],
   "source": [
    "# province_name\n",
    "province_names = [r.strip().lower() for r in ds_no_audio[\"train\"][\"province_name\"]]\n",
    "\n",
    "counter = Counter(province_names)\n",
    "\n",
    "print(\"Unique provinces: \", len(counter))\n",
    "for name, count in counter.most_common():\n",
    "    print(f\"{name}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "188c0335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['region', 'province_code', 'province_name', 'filename', 'text', 'speakerID', 'gender', 'audio'],\n",
       "    num_rows: 15023\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_no_audio[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "177ddbb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'region': Value(dtype='string', id=None),\n",
       " 'province_code': Value(dtype='int64', id=None),\n",
       " 'province_name': Value(dtype='string', id=None),\n",
       " 'filename': Value(dtype='string', id=None),\n",
       " 'text': Value(dtype='string', id=None),\n",
       " 'speakerID': Value(dtype='string', id=None),\n",
       " 'gender': Value(dtype='int64', id=None),\n",
       " 'audio': Audio(sampling_rate=None, mono=True, decode=False, id=None)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_no_audio[\"train\"].features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04553c7e",
   "metadata": {},
   "source": [
    "## Assign Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08f66b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # collect unique province names from train split\n",
    "# all_provinces = sorted(set([p.strip().lower() for p in ds[\"train\"][\"province_name\"] if p is not None]))\n",
    "# province2label = {name: idx for idx, name in enumerate(all_provinces)}\n",
    "# print(\"Mapping:\", province2label)\n",
    "province2label = {\n",
    "    \"hanoi\": 0,\n",
    "    \"hanam\": 0,\n",
    "    \"namdinh\": 0,\n",
    "    \"ninhbinh\": 0,\n",
    "    \"nghean\": 1,\n",
    "    \"thanhhoa\": 1,\n",
    "    \"hatinh\": 1,\n",
    "    \"quangbinh\": 2,\n",
    "    \"quangtri\": 2,\n",
    "    \"thuathienhue\": 2,\n",
    "    \"danang\": 3,\n",
    "    \"quangnam\": 3,\n",
    "    \"quangngai\": 3,\n",
    "    \"hochiminh\": 4,\n",
    "    \"binhduong\": 4,\n",
    "    \"bariavungtau\": 4,\n",
    "    \"dongnai\": 4,\n",
    "    \"cantho\": 5,\n",
    "    \"angiang\": 5,\n",
    "    \"kiengiang\": 5,\n",
    "    \"camau\": 5\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90c655b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_label_batch(batch):\n",
    "    labels = []\n",
    "    for name in batch[\"province_name\"]:\n",
    "        if name is None:\n",
    "            labels.append(-1)\n",
    "        else:\n",
    "            labels.append(province2label.get(name.strip().lower(), -1))\n",
    "    batch[\"accent_label\"] = [int(x) for x in labels]\n",
    "    return batch\n",
    "\n",
    "def process_dataset(ds):\n",
    "    ds = ds.map(\n",
    "        add_label_batch,\n",
    "        batched=True,\n",
    "        batch_size=128, # smaller chunk, too big (1000) causes Arrow to crash ;-;\n",
    "        num_proc=1,\n",
    "        remove_columns=[\"region\", \"province_name\", \"province_code\", \"speakerID\"]\n",
    "    )\n",
    "    ds = ds.filter(lambda x: x[\"accent_label\"] != -1, num_proc=1)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "442bd179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['filename', 'text', 'gender', 'audio', 'accent_label'],\n",
      "        num_rows: 5041\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['filename', 'text', 'gender', 'audio', 'accent_label'],\n",
      "        num_rows: 674\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['filename', 'text', 'gender', 'audio', 'accent_label'],\n",
      "        num_rows: 661\n",
      "    })\n",
      "})\n",
      "{'filename': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'gender': Value(dtype='int64', id=None), 'audio': Audio(sampling_rate=None, mono=True, decode=True, id=None), 'accent_label': Value(dtype='int64', id=None)}\n"
     ]
    }
   ],
   "source": [
    "ds_proccessed = process_dataset(ds)\n",
    "print(ds_proccessed)\n",
    "print(ds_proccessed[\"train\"].features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "320058c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_ds_file = \"data/processed_dataset.pkl\"\n",
    "\n",
    "with open(processed_ds_file, \"wb\") as file:\n",
    "    pickle.dump(ds_proccessed, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
