{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b53460f7",
   "metadata": {},
   "source": [
    "## Import and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb75feb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current working directory is: /home/thun/Documents/python_pj/accent_vn/notebooks\n",
      "The current working directory is: /home/thun/Documents/python_pj/accent_vn\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "print(f\"The current working directory is: {current_directory}\")\n",
    "\n",
    "os.chdir(\"..\")\n",
    "current_directory = os.getcwd()\n",
    "print(f\"The current working directory is: {current_directory}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "110724a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thun/Documents/python_pj/accent_vn/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['region', 'province_code', 'province_name', 'filename', 'text', 'speakerID', 'gender', 'audio'],\n",
      "        num_rows: 15023\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['region', 'province_code', 'province_name', 'filename', 'text', 'speakerID', 'gender', 'audio'],\n",
      "        num_rows: 2026\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['region', 'province_code', 'province_name', 'filename', 'text', 'speakerID', 'gender', 'audio'],\n",
      "        num_rows: 1900\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "ds = load_from_disk(\"my_dataset\")\n",
    "\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09f25f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "region: <class 'str'>\n",
      "province_code: <class 'int'>\n",
      "province_name: <class 'str'>\n",
      "filename: <class 'str'>\n",
      "text: <class 'str'>\n",
      "speakerID: <class 'str'>\n",
      "gender: <class 'int'>\n",
      "audio: <class 'dict'>\n",
      "Audio features:  dict_keys(['path', 'array', 'sampling_rate'])\n"
     ]
    }
   ],
   "source": [
    "train_ds = ds[\"train\"]\n",
    "example = train_ds[0]\n",
    "\n",
    "for k, v in example.items():\n",
    "    print(f\"{k}: {type(v)}\")\n",
    "\n",
    "print(\"Audio features: \", example[\"audio\"].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ea2238f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'region': 'North',\n",
       " 'province_code': 11,\n",
       " 'province_name': 'CaoBang',\n",
       " 'filename': '11_0001.wav',\n",
       " 'text': 'Nghiên cứu học tập,các ứng dụng các khoa học công nghệ và những dụng tiên tiến để đưa vào trong áp dụng trong công việc của mình. Từ đó giúp chuyển đổi những cái khó khăn trong công việc, đưa ra những thuận lợi và những cái nhanh gọn hơn giải quyết các thủ tục hành chính cho được thuận tiện hơn.',\n",
       " 'speakerID': 'spk_11_0001',\n",
       " 'gender': 1,\n",
       " 'audio': {'path': '11_0001.wav',\n",
       "  'array': array([ 0.01013184,  0.00985718,  0.00961304, ..., -0.0005188 ,\n",
       "         -0.0005188 , -0.00033569], shape=(1069160,)),\n",
       "  'sampling_rate': 44100}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab755582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0+cu121\n",
      "12.1\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import torch #torch==2.3.0\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "826ee074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'north': 5913, 'central': 4705, 'south': 4405})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Audio\n",
    "from collections import Counter\n",
    "\n",
    "# trecast audio no decoding\n",
    "ds_no_audio = ds.cast_column(\"audio\", Audio(decode=False))\n",
    "\n",
    "# region\n",
    "regions = [r.strip().lower() for r in ds_no_audio[\"train\"][\"region\"]]\n",
    "print(Counter(regions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1453999e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique provinces:  63\n",
      "caobang: 287\n",
      "nghean: 280\n",
      "quangbinh: 280\n",
      "danang: 271\n",
      "thaibinh: 265\n",
      "binhthuan: 262\n",
      "hagiang: 260\n",
      "quangngai: 260\n",
      "binhphuoc: 260\n",
      "bariavungtau: 255\n",
      "gialai: 252\n",
      "thainguyen: 251\n",
      "quangninh: 249\n",
      "binhduong: 249\n",
      "phutho: 248\n",
      "hanoi: 248\n",
      "hochiminh: 246\n",
      "ninhthuan: 246\n",
      "haugiang: 245\n",
      "daklak: 244\n",
      "daknong: 244\n",
      "longan: 243\n",
      "phuyen: 242\n",
      "kontum: 242\n",
      "haiphong: 241\n",
      "laocai: 241\n",
      "sonla: 241\n",
      "binhdinh: 241\n",
      "langson: 240\n",
      "namdinh: 240\n",
      "thuathienhue: 240\n",
      "lamdong: 239\n",
      "hatinh: 238\n",
      "tayninh: 237\n",
      "thanhhoa: 236\n",
      "camau: 236\n",
      "laichau: 235\n",
      "quangtri: 233\n",
      "yenbai: 232\n",
      "haiduong: 231\n",
      "bentre: 231\n",
      "quangnam: 230\n",
      "backan: 229\n",
      "tiengiang: 228\n",
      "vinhlong: 226\n",
      "dienbien: 225\n",
      "khanhhoa: 225\n",
      "travinh: 225\n",
      "tuyenquang: 224\n",
      "baclieu: 224\n",
      "hungyen: 221\n",
      "bacninh: 221\n",
      "hoabinh: 220\n",
      "angiang: 220\n",
      "kiengiang: 220\n",
      "ninhbinh: 218\n",
      "dongnai: 218\n",
      "dongthap: 217\n",
      "bacgiang: 217\n",
      "soctrang: 216\n",
      "vinhphuc: 215\n",
      "hanam: 214\n",
      "cantho: 209\n"
     ]
    }
   ],
   "source": [
    "# province_name\n",
    "province_names = [r.strip().lower() for r in ds_no_audio[\"train\"][\"province_name\"]]\n",
    "\n",
    "counter = Counter(province_names)\n",
    "\n",
    "print(\"Unique provinces: \", len(counter))\n",
    "for name, count in counter.most_common():\n",
    "    print(f\"{name}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "188c0335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['region', 'province_code', 'province_name', 'filename', 'text', 'speakerID', 'gender', 'audio'],\n",
       "    num_rows: 15023\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_no_audio[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "177ddbb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'region': Value(dtype='string', id=None),\n",
       " 'province_code': Value(dtype='int64', id=None),\n",
       " 'province_name': Value(dtype='string', id=None),\n",
       " 'filename': Value(dtype='string', id=None),\n",
       " 'text': Value(dtype='string', id=None),\n",
       " 'speakerID': Value(dtype='string', id=None),\n",
       " 'gender': Value(dtype='int64', id=None),\n",
       " 'audio': Audio(sampling_rate=None, mono=True, decode=False, id=None)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_no_audio[\"train\"].features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04553c7e",
   "metadata": {},
   "source": [
    "## Assign Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08f66b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # collect unique province names from train split\n",
    "# all_provinces = sorted(set([p.strip().lower() for p in ds[\"train\"][\"province_name\"] if p is not None]))\n",
    "# province2label = {name: idx for idx, name in enumerate(all_provinces)}\n",
    "# print(\"Mapping:\", province2label)\n",
    "province2label = {\n",
    "    \"hanoi\": 0,\n",
    "    \"hanam\": 0,\n",
    "    \"namdinh\": 0,\n",
    "    \"ninhbinh\": 0,\n",
    "    \"nghean\": 1,\n",
    "    \"thanhhoa\": 1,\n",
    "    \"hatinh\": 1,\n",
    "    \"quangbinh\": 2,\n",
    "    \"quangtri\": 2,\n",
    "    \"thuathienhue\": 2,\n",
    "    \"danang\": 3,\n",
    "    \"quangnam\": 3,\n",
    "    \"quangngai\": 3,\n",
    "    \"hochiminh\": 4,\n",
    "    \"binhduong\": 4,\n",
    "    \"bariavungtau\": 4,\n",
    "    \"dongnai\": 4,\n",
    "    \"cantho\": 5,\n",
    "    \"angiang\": 5,\n",
    "    \"kiengiang\": 5,\n",
    "    \"camau\": 5\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90c655b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_label_batch(batch):\n",
    "    labels = []\n",
    "    for name in batch[\"province_name\"]:\n",
    "        if name is None:\n",
    "            labels.append(-1)\n",
    "        else:\n",
    "            labels.append(province2label.get(name.strip().lower(), -1))\n",
    "    batch[\"accent_label\"] = [int(x) for x in labels]\n",
    "    return batch\n",
    "\n",
    "def process_dataset(ds):\n",
    "    ds = ds.map(\n",
    "        add_label_batch,\n",
    "        batched=True,\n",
    "        batch_size=128, # smaller chunk, too big (1000) causes Arrow to crash ;-;\n",
    "        num_proc=1,\n",
    "        remove_columns=[\"region\", \"province_name\", \"province_code\", \"speakerID\", \"filename\"]\n",
    "    )\n",
    "    ds = ds.filter(lambda x: x[\"accent_label\"] != -1, num_proc=1)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "442bd179",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 15023/15023 [04:49<00:00, 51.83 examples/s]\n",
      "Filter: 100%|██████████| 2026/2026 [00:34<00:00, 58.43 examples/s]\n",
      "Filter: 100%|██████████| 1900/1900 [00:32<00:00, 58.39 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'gender', 'audio', 'accent_label'],\n",
      "        num_rows: 5041\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'gender', 'audio', 'accent_label'],\n",
      "        num_rows: 674\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['text', 'gender', 'audio', 'accent_label'],\n",
      "        num_rows: 661\n",
      "    })\n",
      "})\n",
      "{'text': Value(dtype='string', id=None), 'gender': Value(dtype='int64', id=None), 'audio': Audio(sampling_rate=None, mono=True, decode=True, id=None), 'accent_label': Value(dtype='int64', id=None)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (35/35 shards): 100%|██████████| 5041/5041 [02:57<00:00, 28.42 examples/s]\n",
      "Saving the dataset (5/5 shards): 100%|██████████| 674/674 [00:21<00:00, 31.73 examples/s]\n",
      "Saving the dataset (5/5 shards): 100%|██████████| 661/661 [00:29<00:00, 22.24 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ds_proccessed = process_dataset(ds)\n",
    "print(ds_proccessed)\n",
    "print(ds_proccessed[\"train\"].features)\n",
    "\n",
    "ds_proccessed.save_to_disk(\"processed_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b0d9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (35/35 shards): 100%|██████████| 5041/5041 [03:35<00:00, 23.43 examples/s]\n",
      "Saving the dataset (5/5 shards): 100%|██████████| 674/674 [00:29<00:00, 22.71 examples/s]\n",
      "Saving the dataset (5/5 shards): 100%|██████████| 661/661 [00:32<00:00, 20.09 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# resample from 44k1 to 16k\n",
    "ds_proccessed16k = ds_proccessed.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "ds_proccessed16k.save_to_disk(\"processed_dataset16k\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320058c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "processed_ds_file = \"data/processed_ds/vimd_ds.pkl\"\n",
    "processed_ds16k_file = \"data/processed_ds/vimd_ds16kHz.pkl\"\n",
    "\n",
    "with open(processed_ds_file, \"wb\") as file:\n",
    "    pickle.dump(ds_proccessed, file)\n",
    "    \n",
    "with open(processed_ds16k_file, \"wb\") as file:\n",
    "    pickle.dump(ds_proccessed16k, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
